{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Setup Environment for OpenAI\n",
    "\n",
    "Store `OPENAI_API_KEY` as an environment variable in a `.env` file and load it using [pytyon-dotenv](https://pypi.org/project/python-dotenv/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "gpt_model_name = \"gpt-3.5-turbo-1106\"\n",
    "client = OpenAI(\n",
    "    api_key=os.environ['OPENAI_API_KEY'],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Create functions to access OpenAI API's\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transcript(file_path):\n",
    "    audio_file= open(file_path, \"rb\")\n",
    "    transcript = client.audio.transcriptions.create(\n",
    "        model=\"whisper-1\",\n",
    "        file=audio_file,\n",
    "        response_format=\"text\"\n",
    "    )\n",
    "    return transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are an experienced English tutor who graduated from Harvard University in Boston.\n",
    "You are talking to a student who wants to practice speaking English. \n",
    "Help them practice speaking English by talking to your student and \n",
    "try to teach your student how to say what they would like to say.\n",
    "The answer must be formatted as a JSON string\n",
    "\"\"\"\n",
    "\n",
    "def get_gpt_response(transcript, history):\n",
    "  \n",
    "  system_message = {\n",
    "    \"role\": \"system\", \n",
    "    \"content\": system_prompt.replace(\"\\n\", \" \")\n",
    "  }\n",
    "  \n",
    "  message_list = [system_message]\n",
    "  message_list.extend(history)\n",
    "  message_list.append({\"role\": \"user\", \"content\": transcript})\n",
    "\n",
    "  response = client.chat.completions.create(\n",
    "    model=gpt_model_name,\n",
    "    response_format={ \"type\": \"json_object\" },\n",
    "    messages=message_list\n",
    "  )\n",
    "  \n",
    "  return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from playsound import playsound\n",
    "\n",
    "speech_file_path = \"./speech.wav\"\n",
    "\n",
    "def play_gpt_response_with_tts(gpt_response):\n",
    "    response = client.audio.speech.create(\n",
    "        model=\"tts-1\",\n",
    "        voice=\"alloy\",\n",
    "        input=gpt_response\n",
    "    )\n",
    "\n",
    "    response.stream_to_file(speech_file_path)\n",
    "    playsound(speech_file_path)\n",
    "    os.remove(speech_file_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "history = []\n",
    "\n",
    "\n",
    "def talk_to_gpt(file_path):\n",
    "    user_transcript = get_transcript(file_path)\n",
    "    gpt_response = get_gpt_response(user_transcript, history[-10:])\n",
    "    gpt_response = json.loads(gpt_response)\n",
    "    gpt_response = gpt_response['response']\n",
    "    history.extend([\n",
    "        {\"role\": \"user\", \"content\": user_transcript}, \n",
    "        {\"role\": \"assistant\", \"content\": gpt_response}\n",
    "    ])\n",
    "    print(gpt_response)\n",
    "    play_gpt_response_with_tts(gpt_response=gpt_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import wavio\n",
    "\n",
    "class AudioRecorder:\n",
    "    def __init__(self):\n",
    "        self.is_recording = False\n",
    "        self.audio_data = []\n",
    "        self.fs = 44100  # Sample rate\n",
    "        self.channels = 1  # Channels\n",
    "\n",
    "    def start_recording(self):\n",
    "        self.is_recording = True\n",
    "        self.audio_data = []\n",
    "        threading.Thread(target=self.record).start()\n",
    "\n",
    "    def stop_recording(self):\n",
    "        self.is_recording = False\n",
    "\n",
    "    def record(self):\n",
    "        with sd.InputStream(samplerate=self.fs, channels=self.channels) as stream:\n",
    "            while self.is_recording:\n",
    "                data, _ = stream.read(1024)\n",
    "                self.audio_data.append(data)\n",
    "\n",
    "    def save(self, filename='output.wav'):\n",
    "        if self.audio_data:\n",
    "            wav_data = np.concatenate(self.audio_data, axis=0)\n",
    "            wavio.write(filename, wav_data, self.fs, sampwidth=2)\n",
    "            print(\"Recording saved to\", filename)\n",
    "            return filename\n",
    "        else:\n",
    "            print(\"No recording data to save.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7419440f05f44fce8810c90e824146f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Start Recording', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc6fd129f0294310b0e9274983711e58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Stop Recording', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording started...\n",
      "Recording saved to output.wav\n",
      "Hello! I'm doing well, thank you. How are you today?\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "File format b'\\xff\\xf3\\xe4\\xc4' not understood. Only 'RIFF' and 'RIFX' supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 17\u001b[0m, in \u001b[0;36mon_stop_clicked\u001b[0;34m(b)\u001b[0m\n\u001b[1;32m     15\u001b[0m recorder\u001b[38;5;241m.\u001b[39mstop_recording()\n\u001b[1;32m     16\u001b[0m file_name \u001b[38;5;241m=\u001b[39m recorder\u001b[38;5;241m.\u001b[39msave()\n\u001b[0;32m---> 17\u001b[0m \u001b[43mtalk_to_gpt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecording stopped and saved.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 16\u001b[0m, in \u001b[0;36mtalk_to_gpt\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m     11\u001b[0m history\u001b[38;5;241m.\u001b[39mextend([\n\u001b[1;32m     12\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: user_transcript}, \n\u001b[1;32m     13\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: gpt_response}\n\u001b[1;32m     14\u001b[0m ])\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(gpt_response)\n\u001b[0;32m---> 16\u001b[0m \u001b[43mplay_gpt_response_with_tts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgpt_response\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgpt_response\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 18\u001b[0m, in \u001b[0;36mplay_gpt_response_with_tts\u001b[0;34m(gpt_response)\u001b[0m\n\u001b[1;32m     15\u001b[0m response\u001b[38;5;241m.\u001b[39mstream_to_file(speech_file_path)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# playsound(speech_file_path)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Read the WAV file\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m fs, data \u001b[38;5;241m=\u001b[39m \u001b[43mwavfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspeech_file_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Play the audio\u001b[39;00m\n\u001b[1;32m     21\u001b[0m sd\u001b[38;5;241m.\u001b[39mplay(data, fs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/envs/english-tutor/lib/python3.10/site-packages/scipy/io/wavfile.py:650\u001b[0m, in \u001b[0;36mread\u001b[0;34m(filename, mmap)\u001b[0m\n\u001b[1;32m    647\u001b[0m     fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    649\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 650\u001b[0m     file_size, is_big_endian \u001b[38;5;241m=\u001b[39m \u001b[43m_read_riff_chunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfid\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    651\u001b[0m     fmt_chunk_received \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    652\u001b[0m     data_chunk_received \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.12/envs/english-tutor/lib/python3.10/site-packages/scipy/io/wavfile.py:521\u001b[0m, in \u001b[0;36m_read_riff_chunk\u001b[0;34m(fid)\u001b[0m\n\u001b[1;32m    518\u001b[0m     fmt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m>I\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    520\u001b[0m     \u001b[38;5;66;03m# There are also .wav files with \"FFIR\" or \"XFIR\" signatures?\u001b[39;00m\n\u001b[0;32m--> 521\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile format \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(str1)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not understood. Only \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    522\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRIFF\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRIFX\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    524\u001b[0m \u001b[38;5;66;03m# Size of entire file\u001b[39;00m\n\u001b[1;32m    525\u001b[0m file_size \u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39munpack(fmt, fid\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;241m4\u001b[39m))[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m8\u001b[39m\n",
      "\u001b[0;31mValueError\u001b[0m: File format b'\\xff\\xf3\\xe4\\xc4' not understood. Only 'RIFF' and 'RIFX' supported."
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording started...\n",
      "Recording saved to output.wav\n",
      "I am currently helping you practice speaking English. Is there something specific you would like to work on?\n",
      "Recording stopped and saved.\n"
     ]
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "recorder = AudioRecorder()\n",
    "\n",
    "start_button = widgets.Button(description=\"Start Recording\")\n",
    "stop_button = widgets.Button(description=\"Stop Recording\")\n",
    "\n",
    "def on_start_clicked(b):\n",
    "    recorder.start_recording()\n",
    "    print(\"Recording started...\")\n",
    "\n",
    "def on_stop_clicked(b):\n",
    "    print(\"Recording stopped and saved.\")\n",
    "    recorder.stop_recording()\n",
    "    file_name = recorder.save()\n",
    "    talk_to_gpt(file_name)\n",
    "    os.remove(file_name)\n",
    "\n",
    "start_button.on_click(on_start_clicked)\n",
    "stop_button.on_click(on_stop_clicked)\n",
    "\n",
    "display(start_button, stop_button)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
