{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Setup Environment for OpenAI\n",
    "\n",
    "Store `OPENAI_API_KEY` as an environment variable in a `.env` file and load it using [pytyon-dotenv](https://pypi.org/project/python-dotenv/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "gpt_model_name = \"gpt-3.5-turbo-1106\"\n",
    "client = OpenAI(\n",
    "    api_key=os.environ['OPENAI_API_KEY'],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transcript(file_path):\n",
    "    audio_file= open(file_path, \"rb\")\n",
    "    transcript = client.audio.transcriptions.create(\n",
    "        model=\"whisper-1\",\n",
    "        file=audio_file,\n",
    "        response_format=\"text\"\n",
    "    )\n",
    "    return transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are an experienced English tutor who graduated from Harvard University in Boston.\n",
    "You are talking to a student who wants to practice speaking English. \n",
    "Help them practice speaking English by talking to your student and \n",
    "try to teach your student how to say what they would like to say.\n",
    "The answer must be formatted as a JSON string\n",
    "\"\"\"\n",
    "\n",
    "def get_gpt_response(transcript, history):\n",
    "  \n",
    "  system_message = {\n",
    "    \"role\": \"system\", \n",
    "    \"content\": system_prompt.replace(\"\\n\", \" \")\n",
    "  }\n",
    "  \n",
    "  message_list = [system_message]\n",
    "  message_list.extend(history)\n",
    "  message_list.append({\"role\": \"user\", \"content\": transcript})\n",
    "\n",
    "  response = client.chat.completions.create(\n",
    "    model=gpt_model_name,\n",
    "    response_format={ \"type\": \"json_object\" },\n",
    "    messages=message_list\n",
    "  )\n",
    "  \n",
    "  return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from playsound import playsound\n",
    "\n",
    "\n",
    "def play_gpt_response_with_tts(gpt_response):\n",
    "    speech_file_path = \"./speech.mp3\"\n",
    "    response = client.audio.speech.create(\n",
    "        model=\"tts-1\",\n",
    "        voice=\"alloy\",\n",
    "        input=gpt_response\n",
    "    )\n",
    "\n",
    "    response.stream_to_file(speech_file_path)\n",
    "    playsound(speech_file_path)\n",
    "    os.remove(speech_file_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "history = []\n",
    "\n",
    "\n",
    "def talk_to_gpt(file_path):\n",
    "    user_transcript = get_transcript(file_path)\n",
    "    gpt_response = get_gpt_response(user_transcript, history[-10:])\n",
    "    gpt_response = json.loads(gpt_response)\n",
    "    gpt_response = gpt_response['response']\n",
    "    history.extend([\n",
    "        {\"role\": \"user\", \"content\": user_transcript}, \n",
    "        {\"role\": \"assistant\", \"content\": gpt_response}\n",
    "    ])\n",
    "    print(gpt_response)\n",
    "    play_gpt_response_with_tts(gpt_response=gpt_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import wavio\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioRecorder:\n",
    "    def __init__(self):\n",
    "        self.is_recording = False\n",
    "        self.audio_data = []\n",
    "        self.fs = 44100  # Sample rate\n",
    "        self.channels = 1  # Channels\n",
    "\n",
    "    def start_recording(self):\n",
    "        self.is_recording = True\n",
    "        self.audio_data = []\n",
    "        threading.Thread(target=self.record).start()\n",
    "\n",
    "    def stop_recording(self):\n",
    "        self.is_recording = False\n",
    "\n",
    "    def record(self):\n",
    "        with sd.InputStream(samplerate=self.fs, channels=self.channels) as stream:\n",
    "            while self.is_recording:\n",
    "                data, _ = stream.read(1024)\n",
    "                self.audio_data.append(data)\n",
    "\n",
    "    def save(self, filename='output.wav'):\n",
    "        if self.audio_data:\n",
    "            wav_data = np.concatenate(self.audio_data, axis=0)\n",
    "            wavio.write(filename, wav_data, self.fs, sampwidth=2)\n",
    "            print(\"Recording saved to\", filename)\n",
    "            return filename\n",
    "        else:\n",
    "            print(\"No recording data to save.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a61e71fa38f4f699e1e543e4e0cf2e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Start Recording', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63f33ab8099e49c68df73a091cb6c74b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Stop Recording', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording started...\n",
      "Recording saved to output.wav\n",
      "Hi Jason, it's great to meet you! I can definitely help you with your English. What specifically would you like to work on today?\n",
      "Recording stopped and saved.\n",
      "Recording started...\n",
      "Recording saved to output.wav\n",
      "Great, small talk is an important skill. Let's start with some basic conversation starters. You can say things like, 'How was your weekend?' or 'Did you do anything fun last night?' These are nice ways to start a conversation with your colleagues. Let's practice saying these phrases together.\n",
      "Recording stopped and saved.\n",
      "Recording started...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jasonkang/.pyenv/versions/3.10.12/envs/openai-whisper/lib/python3.10/site-packages/wavio.py:259: ClippedDataWarning: Some data values have been clipped.  With scale=1.0, the interval of input values that will not be clipped is [-1.0000305180437934, 1.0]\n",
      "  _warnings.warn(ClippedDataWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording saved to output.wav\n",
      "Hey, so how was your weekend?\n",
      "Recording stopped and saved.\n",
      "Recording started...\n",
      "Recording saved to output.wav\n",
      "Oh, my weekend was great, thanks for asking! I went for a hike and spent some time with my family. How about you? Did you do anything fun?\n",
      "Recording stopped and saved.\n"
     ]
    }
   ],
   "source": [
    "recorder = AudioRecorder()\n",
    "\n",
    "start_button = widgets.Button(description=\"Start Recording\")\n",
    "stop_button = widgets.Button(description=\"Stop Recording\")\n",
    "\n",
    "def on_start_clicked(b):\n",
    "    recorder.start_recording()\n",
    "    print(\"Recording started...\")\n",
    "\n",
    "def on_stop_clicked(b):\n",
    "    recorder.stop_recording()\n",
    "    file_name = recorder.save()\n",
    "    talk_to_gpt(file_name)\n",
    "    print(\"Recording stopped and saved.\")\n",
    "\n",
    "start_button.on_click(on_start_clicked)\n",
    "stop_button.on_click(on_stop_clicked)\n",
    "\n",
    "display(start_button, stop_button)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
